import "@typespec/rest";
import "@typespec/http";

using TypeSpec.Rest;
using TypeSpec.Http;

namespace Azure.AI.OpenAI;

model Paths1Vtxb06DeploymentsDeploymentIdCompletionsPostRequestbodyContentApplicationJsonSchema {
  @doc("""
An optional prompt to complete from, encoded as a string, a list of strings, or
a list of token lists. Defaults to <|endoftext|>. The prompt to complete from.
If you would like to provide multiple prompts, use the POST variant of this
method. Note that <|endoftext|> is the document separator that the model sees
during training, so if a prompt is not specified the model will generate as if
from the beginning of a new document. Maximum allowed size of string list is
2048.
""")
  prompt?: PostContentSchemaPrompt;

  @doc("The maximum number of tokens to generate. Has minimum of 0.")
  @projectedName("json", "max_tokens")
  maxTokens?: int32;

  @doc("""
What sampling temperature to use. Higher values means the model will take more
risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones
with a well-defined answer.
We generally recommend using this or `top_p` but
not both.
Minimum of 0 and maximum of 2 allowed.

""")
  temperature?: float32;

  @doc("""
An alternative to sampling with temperature, called nucleus sampling, where the
model considers the results of the tokens with top_p probability mass. So 0.1
means only the tokens comprising the top 10% probability mass are
considered.
We generally recommend using this or `temperature` but not
both.
Minimum of 0 and maximum of 1 allowed.

""")
  @projectedName("json", "top_p")
  topP?: float32;

  @doc("""
Defaults to null. Modify the likelihood of specified tokens appearing in the
completion. Accepts a json object that maps tokens (specified by their token ID
in the GPT tokenizer) to an associated bias value from -100 to 100. You can use
this tokenizer tool (which works for both GPT-2 and GPT-3) to convert text to
token IDs. Mathematically, the bias is added to the logits generated by the
model prior to sampling. The exact effect will vary per model, but values
between -1 and 1 should decrease or increase likelihood of selection; values
like -100 or 100 should result in a ban or exclusive selection of the relevant
token. As an example, you can pass {\"50256\" &#58; -100} to prevent the
<|endoftext|> token from being generated.
""")
  @projectedName("json", "logit_bias")
  logitBias?: unknown;

  @doc("The ID of the end-user, for use in tracking and rate-limiting.")
  user?: string;

  @doc("""
How many snippets to generate for each prompt. Minimum of 1 and maximum of 128
allowed.
""")
  n?: int32;

  @doc("""
Whether to enable streaming for this endpoint. If set, tokens will be sent as
server-sent events as they become available.
""")
  stream?: boolean;

  @doc("""
Include the log probabilities on the `logprobs` most likely tokens, as well the
chosen tokens. So for example, if `logprobs` is 10, the API will return a list
of the 10 most likely tokens. If `logprobs` is 0, only the chosen tokens will
have logprobs returned. Minimum of 0 and maximum of 100 allowed.
""")
  logprobs?: int32;

  @doc("The name of the model to use")
  `model`?: string;

  @doc("Echo back the prompt in addition to the completion")
  echo?: boolean;

  @doc("A sequence which indicates the end of the current document.")
  stop?: PostContentSchemaStop;

  @projectedName("json", "completion_config")
  completionConfig?: string;

  @doc("""
can be used to disable any server-side caching, 0=no cache, 1=prompt prefix
enabled, 2=full cache
""")
  @projectedName("json", "cache_level")
  cacheLevel?: int32;

  @doc("""
How much to penalize new tokens based on their existing frequency in the text
so far. Decreases the model's likelihood to repeat the same line verbatim. Has
minimum of -2 and maximum of 2.
""")
  @projectedName("json", "presence_penalty")
  presencePenalty?: float32;

  @doc("""
How much to penalize new tokens based on whether they appear in the text so
far. Increases the model's likelihood to talk about new topics.
""")
  @projectedName("json", "frequency_penalty")
  frequencyPenalty?: float32;

  @doc("""
How many generations to create server side, and display only the best. Will not
stream intermediate progress if best_of > 1. Has maximum value of 128.
""")
  @projectedName("json", "best_of")
  bestOf?: int32;
}

@doc("""
An optional prompt to complete from, encoded as a string, a list of strings, or
a list of token lists. Defaults to <|endoftext|>. The prompt to complete from.
If you would like to provide multiple prompts, use the POST variant of this
method. Note that <|endoftext|> is the document separator that the model sees
during training, so if a prompt is not specified the model will generate as if
from the beginning of a new document. Maximum allowed size of string list is
2048.
""")
model PostContentSchemaPrompt {}

@doc("A sequence which indicates the end of the current document.")
model PostContentSchemaStop {}

model PathsMaorw9DeploymentsDeploymentIdCompletionsPostResponses200ContentApplicationJsonSchema {
  id?: string;
  object?: string;
  created?: int32;
  `model`?: string;
  choices?: Post200ApplicationJsonPropertiesItemsItem[];
}

model Post200ApplicationJsonPropertiesItemsItem {
  text?: string;
  index?: int32;
  logprobs?: PostResponses200ContentApplicationJsonSchemaChoicesItemLogprobs;

  @projectedName("json", "finish_reason")
  finishReason?: string;
}

model PostResponses200ContentApplicationJsonSchemaChoicesItemLogprobs {
  tokens?: string[];

  @projectedName("json", "token_logprobs")
  tokenLogprobs?: float32[];

  @projectedName("json", "top_logprobs")
  topLogprobs?: Record<float32>[];

  @projectedName("json", "text_offset")
  textOffset?: int32[];
}

@error
model ErrorResponse {
  error?: ErrorResponseError;
}

model ErrorResponseError {
  code?: string;
  message?: string;
  param?: string;
  type?: string;
}

model Paths13PiqocDeploymentsDeploymentIdEmbeddingsPostRequestbodyContentApplicationJsonSchema {
  ...Record<unknown>;

  @doc("""
An input to embed, encoded as a string, a list of strings, or a list of token
lists
""")
  input: PostContentSchemaInput;

  @doc("The ID of the end-user, for use in tracking and rate-limiting.")
  user?: string;

  @doc("input type of embedding search to use")
  @projectedName("json", "input_type")
  inputType?: string;

  @doc("ID of the model to use")
  `model`?: string;
}

@doc("""
An input to embed, encoded as a string, a list of strings, or a list of token
lists
""")
model PostContentSchemaInput {}
